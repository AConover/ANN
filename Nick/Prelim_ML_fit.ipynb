{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary ML fit\n",
    "\n",
    "If unfamiliar with tensorflow, I suggest reading the [docs](https://www.tensorflow.org/guide/eager) before diving into this notebook.  However, I will also explain all the tf calls. The following todo is what this notebook is still missing, but I am releasing it now in the interest of timeliness. My next notebook will contain an exploration of other optimization algorithms, both with gradient and without.\n",
    "\n",
    "Todo:\n",
    "1. Estimate variance with replica\n",
    "2. Replace explicit CFFs in network with preceeding dense layer as example of potential global fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload # whenever changes are made to any imported files this will reload them automatically\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BHDVCStf import BHDVCS #modified bhdvcs file\n",
    "bhdvcs = BHDVCS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvcs_xs_newsets_genCFFs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility class/func defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice wrapper over my data to ensure that it always has the parameters in the correct order. It makes things so much easier.  Please feel free to extend and change it, but please post it if you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DvcsData(object):\n",
    "    def __init__(self, df):\n",
    "        self.X = df.loc[:, ['phi_x', 'k', 'QQ', 'x_b', 't', 'F1', 'F2', 'ReH', 'ReE', 'ReHtilde', 'dvcs']]\n",
    "        self.XnoCFF = df.loc[:, ['phi_x', 'k', 'QQ', 'x_b', 't', 'F1', 'F2', 'dvcs']]\n",
    "        self.y = df.loc[:, 'F']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def getSet(self, setNum, itemsInSet=36):\n",
    "        pd.options.mode.chained_assignment = None\n",
    "        subX = self.X.loc[setNum*itemsInSet:(setNum+1)*itemsInSet-1, :]\n",
    "        subX['F'] = self.y.loc[setNum*itemsInSet:(setNum+1)*itemsInSet-1]\n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "        return DvcsData(subX)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a vectorized wrapper of Liliet's TotalUUXS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecF(DvcsData, TotalUUXS):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        data: this should be of type DvcsData\n",
    "        TotalUUXS: this should be the function from F.C\n",
    "    \"\"\"\n",
    "    results = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        results[i] = TotalUUXS(*data.X.loc[i, :])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data container for the loss and cff values at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class savedParams(object):\n",
    "    def __init__(self, numEpochs):\n",
    "        self.savedparams = pd.DataFrame({'Epoch':np.zeros(numEpochs), 'Loss':np.zeros(numEpochs),\n",
    "                                         'ReH':np.zeros(numEpochs), 'ReE':np.zeros(numEpochs),\n",
    "                                         'ReHtilde':np.zeros(numEpochs)})\n",
    "    \n",
    "    def newData(self, epoch, loss, ReH, ReE, ReHtilde):\n",
    "        self.savedparams.loc[epoch, :] = {'Epoch':epoch, 'Loss':loss, 'ReH':ReH, 'ReE':ReE, 'ReHtilde':ReHtilde}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcterr(obs, exp):\n",
    "    return 100*(obs-exp)/exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function to format printouts at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form(tensors): #only works for 1d tensors\n",
    "    return str(np.round(np.array([x.numpy() for x in tensors]), decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check BHDVCStf for accuracy\n",
    "\n",
    "Whenever modifications are made to the BHDVCS python file it should be checked against Liliet's known correct file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/00\n"
     ]
    }
   ],
   "source": [
    "from ROOT import gROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gROOT.LoadMacro('F.C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import F # not vectorized can only receive scalars\n",
    "f = F()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DvcsData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resLiliets = vecF(data, f.TotalUUXS)\n",
    "resTF = bhdvcs.TotalUUXS_curve_fit(np.asarray(data.XnoCFF).T, data.X['ReH'], data.X['ReE'], data.X['ReHtilde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(resTF.numpy() == resLiliets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the values are identical so we can conclude that there are no errors in BHDVCStf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit TotalUUXS With Adam Optimizer\n",
    "\n",
    "This is functionally identical to the $\\chi^2$ minimization in scipy or root, except with Adam instead of levenberg-marquardt or trust-region.  However, it is useful as a demonstration of the TotalUUXS function as a layer in the\n",
    "tensorflow graph.  It is run in eager execution for ease in experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are just using the first set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DvcsData(df)\n",
    "set0 = data.getSet(0)\n",
    "X_train = np.asarray(set0.XnoCFF).T # have to take transpose to get everything to work\n",
    "y_train = np.asarray(set0.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below defines the TotalUUXS layer.  In tensorflow, parameters are decalared as a tf.Variable, so the three compton form factors are set thus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalUUXS(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TotalUUXS, self).__init__(dtype='float64')\n",
    "        self.ReH = tf.Variable(1., dtype='float64', name='ReH') # all compton form factors are set to 1.0 initially\n",
    "        self.ReE = tf.Variable(1., dtype='float64', name='ReE')\n",
    "        self.ReHtilde = tf.Variable(1., dtype='float64', name='ReHtilde')\n",
    "        self.F = BHDVCS()\n",
    "    def call(self, inputs):\n",
    "        return self.F.TotalUUXS(inputs, self.ReH, self.ReE, self.ReHtilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the mean squared error ($\\chi^2$) between the output of the totalUUXS and the F in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is calculated inside a GradientTape, the gradient of each tf.variable w.r.t. the loss can be calculated.  This function returns the gradients of ReH, ReE, and ReHtilde w.r.t. the loss function (i.e. how much the loss changes as each of the compton form factors change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, [model.ReH, model.ReE, model.ReHtilde])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my training loop.  As a general matter, it looks like the picture below. In this case the model is TotalUUXS, the loss is the one we defined, the gradient step is the the apply_gradients line in the cell below, and \"step\" is the next iteration of the loop.  I haven't used any callbacks beyond saving the best compton form factors, and that's implemented directly instead of as a callback.  If your loop gets more complicated, though, one would perhaps benefit from using (tensorflow's training loop and callbacks)[https://www.tensorflow.org/guide/keras/train_and_evaluate].\n",
    "\n",
    "![](https://dzlab.github.io/assets/2019/20190316-training_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, X_train, y_train, lr=5000, when2print=None):\n",
    "    \n",
    "    sv = savedParams(epochs)\n",
    "    model = TotalUUXS()  # Should maybe be refactored\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) # in this case we need a pretty high learning rate\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        grads = grad(model, X_train, y_train)\n",
    "        optimizer.apply_gradients(zip(grads, [model.ReH, model.ReE, model.ReHtilde]), )\n",
    "\n",
    "        epoch_loss = loss(model, X_train, y_train)\n",
    "        sv.newData(i, epoch_loss.numpy(), model.ReH.numpy(), model.ReE.numpy(), model.ReHtilde.numpy())\n",
    "        if when2print:\n",
    "            if i % when2print == 0: # print state every __ epochs\n",
    "                print(\"Loss at epoch {:03d}: {:.5f}\".format(i, epoch_loss),\n",
    "                      \"Grads: \" + form(grads),\n",
    "                      \"ReH, ReE, ReHtilde: \" + form([model.ReH, model.ReE, model.ReHtilde]))\n",
    "    return sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 000: 16447.75923 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 4985.45  4921.98 -4669.43]\n",
      "Loss at epoch 050: 3.56026 Grads: [-0.28 -0.05  0.01] ReH, ReE, ReHtilde: [ -60.39 -122.75   83.87]\n",
      "Loss at epoch 100: 0.09907 Grads: [ 0.03  0.01 -0.  ] ReH, ReE, ReHtilde: [ 24.64 -37.82  -4.63]\n",
      "Loss at epoch 150: 0.00242 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 10.57 -52.04   8.97]\n",
      "Loss at epoch 200: 0.00001 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 12.41 -50.38   7.34]\n",
      "Loss at epoch 250: 0.00001 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 12.54 -50.45   7.22]\n",
      "Loss at epoch 300: 0.00001 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 12.57 -50.64   7.22]\n",
      "Loss at epoch 350: 0.00001 Grads: [-0.  0. -0.] ReH, ReE, ReHtilde: [ 12.61 -50.83   7.22]\n",
      "Loss at epoch 400: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.65 -51.04   7.22]\n",
      "Loss at epoch 450: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.7  -51.26   7.21]\n",
      "Loss at epoch 500: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.74 -51.48   7.21]\n",
      "Loss at epoch 550: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.78 -51.71   7.21]\n",
      "Loss at epoch 600: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.83 -51.95   7.2 ]\n",
      "Loss at epoch 650: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.88 -52.19   7.2 ]\n",
      "Loss at epoch 700: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.92 -52.43   7.2 ]\n",
      "Loss at epoch 750: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 12.97 -52.67   7.19]\n",
      "Loss at epoch 800: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.02 -52.92   7.19]\n",
      "Loss at epoch 850: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.07 -53.16   7.18]\n",
      "Loss at epoch 900: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.11 -53.4    7.18]\n",
      "Loss at epoch 950: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.16 -53.63   7.18]\n",
      "Loss at epoch 1000: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.2  -53.87   7.17]\n",
      "Loss at epoch 1050: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.25 -54.09   7.17]\n",
      "Loss at epoch 1100: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.29 -54.32   7.17]\n",
      "Loss at epoch 1150: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.33 -54.53   7.16]\n",
      "Loss at epoch 1200: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.37 -54.74   7.16]\n",
      "Loss at epoch 1250: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.41 -54.95   7.16]\n",
      "Loss at epoch 1300: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.45 -55.14   7.15]\n",
      "Loss at epoch 1350: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.49 -55.33   7.15]\n",
      "Loss at epoch 1400: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.52 -55.51   7.15]\n",
      "Loss at epoch 1450: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.56 -55.68   7.15]\n",
      "Loss at epoch 1500: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.59 -55.85   7.14]\n",
      "Loss at epoch 1550: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.62 -56.     7.14]\n",
      "Loss at epoch 1600: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.65 -56.15   7.14]\n",
      "Loss at epoch 1650: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.67 -56.29   7.14]\n",
      "Loss at epoch 1700: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.7  -56.41   7.13]\n",
      "Loss at epoch 1750: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.72 -56.54   7.13]\n",
      "Loss at epoch 1800: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.74 -56.65   7.13]\n",
      "Loss at epoch 1850: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.76 -56.75   7.13]\n",
      "Loss at epoch 1900: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.78 -56.85   7.13]\n",
      "Loss at epoch 1950: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.8  -56.94   7.13]\n",
      "Loss at epoch 2000: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.82 -57.02   7.13]\n",
      "Loss at epoch 2050: 0.00001 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 13.83 -57.1    7.13]\n",
      "Loss at epoch 2100: 226.35113 Grads: [-0.22 -0.04  0.01] ReH, ReE, ReHtilde: [ 595.79  524.78 -574.81]\n",
      "Loss at epoch 2150: 1.04634 Grads: [ 0.01  0.   -0.  ] ReH, ReE, ReHtilde: [-25.71 -96.77  46.69]\n",
      "Loss at epoch 2200: 0.00501 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 11.12 -59.98   9.86]\n",
      "Loss at epoch 2250: 0.00001 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 13.76 -57.39   7.23]\n",
      "Loss at epoch 2300: 0.00001 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 13.86 -57.33   7.13]\n",
      "Loss at epoch 2350: 0.00001 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 13.88 -57.36   7.12]\n",
      "Loss at epoch 2400: 0.00001 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 13.89 -57.39   7.12]\n",
      "Loss at epoch 2450: 0.00001 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 13.89 -57.42   7.12]\n",
      "Loss at epoch 2500: 0.00001 Grads: [-0.  0. -0.] ReH, ReE, ReHtilde: [ 13.9  -57.44   7.12]\n",
      "Loss at epoch 2550: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.9  -57.47   7.12]\n",
      "Loss at epoch 2600: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.91 -57.49   7.12]\n",
      "Loss at epoch 2650: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.91 -57.51   7.12]\n",
      "Loss at epoch 2700: 0.00001 Grads: [-0.  0.  0.] ReH, ReE, ReHtilde: [ 13.92 -57.53   7.12]\n",
      "Loss at epoch 2750: 0.00001 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 13.92 -57.55   7.12]\n",
      "Loss at epoch 2800: 313.00204 Grads: [ 0.05  0.01 -0.  ] ReH, ReE, ReHtilde: [ 698.25  626.75 -677.2 ]\n",
      "Loss at epoch 2850: 0.31514 Grads: [ 0.07  0.01 -0.  ] ReH, ReE, ReHtilde: [ -7.79 -79.29  28.83]\n",
      "Loss at epoch 2900: 0.00290 Grads: [-0. -0.  0.] ReH, ReE, ReHtilde: [ 11.85 -59.67   9.2 ]\n",
      "Loss at epoch 2950: 0.00009 Grads: [ 0.  0. -0.] ReH, ReE, ReHtilde: [ 13.57 -57.96   7.48]\n"
     ]
    }
   ],
   "source": [
    "info = training_loop(3000, X_train, y_train when2print=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PctErr of ReH, ReE, ReHtilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "minloss = info.savedparams.loc[info.savedparams['Loss'].idxmin(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReH: 6.623361917190539\n",
      "ReE: 8.475008187218595\n",
      "ReHtilde: -1.868313218395556\n"
     ]
    }
   ],
   "source": [
    "for cff in ['ReH', 'ReE', 'ReHtilde']:\n",
    "    print(cff + \": \" + str(pcterr(minloss[cff], df.loc[0, cff])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits for all Kinematic Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DvcsData(df) # I am repeating this just to make it clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsets = df[\"#Set\"].max()+1\n",
    "\n",
    "pcterrs = pd.DataFrame({\n",
    "  \"ReH\": np.zeros(numsets),\n",
    "  \"ReE\": np.zeros(numsets),\n",
    "  \"ReHtilde\": np.zeros(numsets)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [42:41<00:00, 170.78s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(numsets)):\n",
    "    setI = data.getSet(i)\n",
    "    X_train = np.asarray(setI.XnoCFF).T # have to take transpose to get everything to work\n",
    "    y_train = np.asarray(setI.y)\n",
    "    info = training_loop(5000, X_train, y_train)\n",
    "    minloss = info.savedparams.loc[info.savedparams['Loss'].idxmin(), :]\n",
    "    \n",
    "    for cff in ['ReH', 'ReE', 'ReHtilde']:\n",
    "        pcterrs.loc[i, cff] = pcterr(minloss[cff], df.loc[0, cff])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReH</th>\n",
       "      <th>ReE</th>\n",
       "      <th>ReHtilde</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.841211</td>\n",
       "      <td>8.750153</td>\n",
       "      <td>-1.899631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-23.032953</td>\n",
       "      <td>-26.754834</td>\n",
       "      <td>17.403152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-50.335061</td>\n",
       "      <td>-19.204528</td>\n",
       "      <td>-36.724805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.185883</td>\n",
       "      <td>54.322118</td>\n",
       "      <td>-10.689371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.235227</td>\n",
       "      <td>-6.967373</td>\n",
       "      <td>5.328316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-48.504747</td>\n",
       "      <td>-16.507414</td>\n",
       "      <td>-39.898916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-24.065140</td>\n",
       "      <td>-22.077185</td>\n",
       "      <td>2.563795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.319300</td>\n",
       "      <td>41.979024</td>\n",
       "      <td>10.807291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-5.460770</td>\n",
       "      <td>-3.362521</td>\n",
       "      <td>5.718416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-38.096585</td>\n",
       "      <td>0.895393</td>\n",
       "      <td>-54.397170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-47.874555</td>\n",
       "      <td>-15.612394</td>\n",
       "      <td>-40.503454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-17.305739</td>\n",
       "      <td>-12.407182</td>\n",
       "      <td>-5.600401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-38.976679</td>\n",
       "      <td>-0.386869</td>\n",
       "      <td>-53.731545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-47.675519</td>\n",
       "      <td>-15.347861</td>\n",
       "      <td>-40.214254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-15.135354</td>\n",
       "      <td>-9.407815</td>\n",
       "      <td>-7.983454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ReH        ReE   ReHtilde\n",
       "0    6.841211   8.750153  -1.899631\n",
       "1  -23.032953 -26.754834  17.403152\n",
       "2  -50.335061 -19.204528 -36.724805\n",
       "3   10.185883  54.322118 -10.689371\n",
       "4   -8.235227  -6.967373   5.328316\n",
       "5  -48.504747 -16.507414 -39.898916\n",
       "6  -24.065140 -22.077185   2.563795\n",
       "7    0.319300  41.979024  10.807291\n",
       "8   -5.460770  -3.362521   5.718416\n",
       "9  -38.096585   0.895393 -54.397170\n",
       "10 -47.874555 -15.612394 -40.503454\n",
       "11 -17.305739 -12.407182  -5.600401\n",
       "12 -38.976679  -0.386869 -53.731545\n",
       "13 -47.675519 -15.347861 -40.214254\n",
       "14 -15.135354  -9.407815  -7.983454"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcterrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average percent error for each CFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReH: 25.46964818873308\n",
      "ReE: 16.932177576928304\n",
      "ReHtilde: 22.23093123037195\n"
     ]
    }
   ],
   "source": [
    "for cff in ['ReH', 'ReE', 'ReHtilde']:\n",
    "    print(cff + \": \" + str(np.mean(np.abs(pcterrs[cff]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "These fits are much worse on average than the baseline fit NewsetExtraction.ipynb, which had average absolute standard errors of 9.8%, 10.5%, and 8%, and they took much longer to compute.  This may be fixable, as there are a number of hyperparameters in the Adam optimizer that can be adjusted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py38Root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
